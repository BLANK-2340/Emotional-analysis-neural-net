import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import RandomOverSampler
import ast
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.cuda.amp import autocast, GradScaler
import time
import logging

# Set up logging
logging.basicConfig(filename='training_log.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logging.info("Starting the training process...")

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# Set device and optimize CUDA operations
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
print(f"Using device: {device}")
logging.info(f"Using device: {device}")

# Load the dataset
file_path = r"C:\Users\armaa\Downloads\dataset\MELD\MELD.xlsx"  # Adjust this path as needed
df = pd.read_excel(file_path)
print("Dataset loaded successfully.")
logging.info("Dataset loaded successfully.")
print(f"Initial dataset shape: {df.shape}")  # Debugging statement
logging.debug(f"Initial dataset shape: {df.shape}")

# Convert string representations to lists
for col in ['V1', 'V2', 'V3', 'V4', 'A2']:
    df[col] = df[col].apply(ast.literal_eval)
    print(f"Column '{col}' successfully converted to list format.")  # Debugging statement
    logging.debug(f"Column '{col}' successfully converted to list format.")

# Ensure utterances are strings
df['Utterance'] = df['Utterance'].astype(str)
print("Utterances ensured as string type.")  # Debugging statement
logging.debug("Utterances ensured as string type.")

# Encode emotion labels
le = LabelEncoder()
df['Emotion_encoded'] = le.fit_transform(df['Emotion'])
print("Emotion labels encoded successfully.")  # Debugging statement
logging.debug("Emotion labels encoded successfully.")

# Print dataset information
print("\nDataset Information:")
print(f"Number of samples: {len(df)}")
print(f"Number of features: {len(df.columns)}")
print("\nEmotion distribution:")
print(df['Emotion'].value_counts())
logging.info(f"Dataset Information: Number of samples: {len(df)}, Number of features: {len(df.columns)}")
logging.info(f"Emotion distribution: {df['Emotion'].value_counts().to_dict()}")

# Print feature dimensions
print("\nFeature dimensions:")
print(f"V1: {len(df['V1'].iloc[0])}")
print(f"V2: {len(df['V2'].iloc[0])}")
print(f"V3: {len(df['V3'].iloc[0])}")
print(f"V4: {len(df['V4'].iloc[0])}")
print(f"A2: {len(df['A2'].iloc[0])}")
logging.debug(f"Feature dimensions: V1: {len(df['V1'].iloc[0])}, V2: {len(df['V2'].iloc[0])}, V3: {len(df['V3'].iloc[0])}, V4: {len(df['V4'].iloc[0])}, A2: {len(df['A2'].iloc[0])}")

# Initialize BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
print("\nBERT tokenizer initialized.")
logging.info("BERT tokenizer initialized.")

# Hyperparameters
batch_size = 64
accumulation_steps = 4
max_length = 128

# Function to tokenize text
def tokenize_text(text, max_length=max_length):
    tokenized = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')
    print(f"Tokenized text: {text[:30]}... | Input IDs shape: {tokenized['input_ids'].shape}")  # Debugging statement
    logging.debug(f"Tokenized text: {text[:30]}... | Input IDs shape: {tokenized['input_ids'].shape}")
    return tokenized

# Prepare features and labels
X = df[['Utterance', 'V1', 'V3', 'V4', 'A2']]
y = df['Emotion_encoded']
print("\nFeatures and labels prepared successfully.")  # Debugging statement
logging.info("Features and labels prepared successfully.")

# Oversample to handle class imbalance
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)
print("\nClass distribution after oversampling:")
print(pd.Series(y_resampled).value_counts())
print(f"Resampled dataset shape: {X_resampled.shape}")  # Debugging statement
logging.info(f"Class distribution after oversampling: {pd.Series(y_resampled).value_counts().to_dict()}")
logging.debug(f"Resampled dataset shape: {X_resampled.shape}")

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)
print("\nData split into train and test sets.")
print(f"Training samples: {len(X_train)}")
print(f"Testing samples: {len(X_test)}")
logging.info(f"Data split into train and test sets. Training samples: {len(X_train)}, Testing samples: {len(X_test)}")

# Custom Dataset class
class MELDDataset(Dataset):
    def __init__(self, utterances, v1, v3, v4, a2, labels):
        self.utterances = utterances
        self.v1 = v1
        self.v3 = v3
        self.v4 = v4
        self.a2 = a2
        self.labels = labels
        print(f"Initialized MELDDataset with {len(self.labels)} samples.")  # Debugging statement
        logging.debug(f"Initialized MELDDataset with {len(self.labels)} samples.")

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        utterance = self.utterances.iloc[idx]
        tokenized = tokenize_text(utterance)
        print(f"Fetching sample {idx}")  # Debugging statement
        logging.debug(f"Fetching sample {idx}")
        return {
            'input_ids': tokenized['input_ids'].squeeze(),
            'attention_mask': tokenized['attention_mask'].squeeze(),
            'v1': torch.tensor(self.v1.iloc[idx], dtype=torch.float32),
            'v3': torch.tensor(self.v3.iloc[idx], dtype=torch.float32),
            'v4': torch.tensor(self.v4.iloc[idx], dtype=torch.float32),
            'a2': torch.tensor(self.a2.iloc[idx], dtype=torch.float32),
            'label': torch.tensor(self.labels.iloc[idx], dtype=torch.long)
        }

# Create datasets and dataloaders
train_dataset = MELDDataset(X_train['Utterance'], X_train['V1'], X_train['V3'], X_train['V4'], X_train['A2'], y_train)
test_dataset = MELDDataset(X_test['Utterance'], X_test['V1'], X_test['V3'], X_test['V4'], X_test['A2'], y_test)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

print("\nDataLoaders created successfully.")
print(f"Number of batches in train_loader: {len(train_loader)}")
print(f"Number of batches in test_loader: {len(test_loader)}")
logging.info("DataLoaders created successfully.")
logging.debug(f"Number of batches in train_loader: {len(train_loader)}, Number of batches in test_loader: {len(test_loader)}")

class MultimodalEmotionRecognition(nn.Module):
    def __init__(self, num_classes=7):
        super(MultimodalEmotionRecognition, self).__init__()
        
        # Text Modality
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.text_fc = nn.Sequential(
            nn.Linear(768, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # Audio Modality
        self.audio_v1_fc = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256)
        )
        self.audio_v34_fc = nn.Sequential(
            nn.Linear(50, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128)
        )
        self.audio_combined_fc = nn.Sequential(
            nn.Linear(384, 256),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # Visual Modality
        self.visual_fc = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3)
        )
        
        # Cross-Modal Attention
        self.text_audio_attention = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)
        self.text_visual_attention = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)
        
        # Fusion Layer
        self.fusion_fc = nn.Sequential(
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # Classification Layers
        self.classifier = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        print("MultimodalEmotionRecognition model initialized successfully.")  # Debugging statement
        logging.info("MultimodalEmotionRecognition model initialized successfully.")
    
    def forward(self, input_ids, attention_mask, v1, v3, v4, a2):
        # Text Modality
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        text_features = self.text_fc(bert_output.pooler_output)
        print(f"Text modality features shape: {text_features.shape}")  # Debugging statement
        logging.debug(f"Text modality features shape: {text_features.shape}")
        
        # Audio Modality
        audio_v1 = self.audio_v1_fc(v1)
        audio_v34 = self.audio_v34_fc(torch.cat([v3, v4], dim=1))
        audio_features = self.audio_combined_fc(torch.cat([audio_v1, audio_v34], dim=1))
        print(f"Audio modality features shape: {audio_features.shape}")  # Debugging statement
        logging.debug(f"Audio modality features shape: {audio_features.shape}")
        
        # Visual Modality
        visual_features = self.visual_fc(a2)
        print(f"Visual modality features shape: {visual_features.shape}")  # Debugging statement
        logging.debug(f"Visual modality features shape: {visual_features.shape}")
        
        # Cross-Modal Attention
        text_features_expanded = text_features.unsqueeze(1)
        audio_features_expanded = nn.functional.pad(audio_features.unsqueeze(1), (0, 256))
        visual_features_expanded = nn.functional.pad(visual_features.unsqueeze(1), (0, 256))
        
        text_audio_attention, _ = self.text_audio_attention(text_features_expanded, audio_features_expanded, audio_features_expanded)
        text_visual_attention, _ = self.text_visual_attention(text_features_expanded, visual_features_expanded, visual_features_expanded)
        
        attended_features = text_features + text_audio_attention.squeeze(1) + text_visual_attention.squeeze(1)
        print(f"Cross-modal attended features shape: {attended_features.shape}")  # Debugging statement
        logging.debug(f"Cross-modal attended features shape: {attended_features.shape}")
        
        # Fusion
        fused_features = self.fusion_fc(torch.cat([attended_features, audio_features, visual_features], dim=1))
        print(f"Fused features shape: {fused_features.shape}")  # Debugging statement
        logging.debug(f"Fused features shape: {fused_features.shape}")
        
        # Classification
        logits = self.classifier(fused_features)
        print(f"Logits shape: {logits.shape}")  # Debugging statement
        logging.debug(f"Logits shape: {logits.shape}")
        
        return logits

# Main execution block
if __name__ == "__main__":
    # Instantiate the model
    model = MultimodalEmotionRecognition()
    model = model.to(device)
    print("Model instantiated successfully.")
    logging.info("Model instantiated successfully.")
    print(f"Number of parameters: {sum(p.numel() for p in model.parameters())}")
    logging.info(f"Number of parameters: {sum(p.numel() for p in model.parameters())}")

    # Debug information
    print("\nDebug information:")
    batch = next(iter(train_loader))
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    v1 = batch['v1'].to(device)
    v3 = batch['v3'].to(device)
    v4 = batch['v4'].to(device)
    a2 = batch['a2'].to(device)

    print(f"Input IDs shape: {input_ids.shape}")
    print(f"Attention Mask shape: {attention_mask.shape}")
    print(f"V1 shape: {v1.shape}")
    print(f"V3 shape: {v3.shape}")
    print(f"V4 shape: {v4.shape}")
    print(f"A2 shape: {a2.shape}")
    logging.debug(f"Input IDs shape: {input_ids.shape}, Attention Mask shape: {attention_mask.shape}, V1 shape: {v1.shape}, V3 shape: {v3.shape}, V4 shape: {v4.shape}, A2 shape: {a2.shape}")

    # Forward pass for debugging
    with torch.no_grad():
        with autocast():
            outputs = model(input_ids, attention_mask, v1, v3, v4, a2)
        print(f"Model output shape: {outputs.shape}")
        logging.debug(f"Model output shape: {outputs.shape}")

    # Training function with sophisticated logging and visualization
    def train_model(model, train_loader, val_loader, num_epochs=10):
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(model.parameters(), lr=1e-4)
        scaler = GradScaler()
        train_losses, val_losses = [], []
        train_accuracies, val_accuracies = [], []

        for epoch in range(num_epochs):
            start_time = time.time()
            model.train()
            total_loss = 0.0
            total_correct = 0
            total_samples = 0
            
            for batch_idx, batch in enumerate(train_loader):
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                v1 = batch['v1'].to(device)
                v3 = batch['v3'].to(device)
                v4 = batch['v4'].to(device)
                a2 = batch['a2'].to(device)
                labels = batch['label'].to(device)
                
                optimizer.zero_grad()
                with autocast():
                    outputs = model(input_ids, attention_mask, v1, v3, v4, a2)
                    loss = criterion(outputs, labels)
                
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
                
                total_loss += loss.item() * labels.size(0)
                _, predicted = torch.max(outputs, 1)
                total_correct += (predicted == labels).sum().item()
                total_samples += labels.size(0)
                
                logging.debug(f"Batch {batch_idx + 1}/{len(train_loader)}, Loss: {loss.item():.4f}")
            
            avg_loss = total_loss / total_samples
            accuracy = total_correct / total_samples
            train_losses.append(avg_loss)
            train_accuracies.append(accuracy)
            logging.info(f"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}")

            # Validation phase
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    v1 = batch['v1'].to(device)
                    v3 = batch['v3'].to(device)
                    v4 = batch['v4'].to(device)
                    a2 = batch['a2'].to(device)
                    labels = batch['label'].to(device)

                    with autocast():
                        outputs = model(input_ids, attention_mask, v1, v3, v4, a2)
                        loss = criterion(outputs, labels)
                    
                    val_loss += loss.item() * labels.size(0)
                    _, predicted = torch.max(outputs, 1)
                    val_correct += (predicted == labels).sum().item()
                    val_total += labels.size(0)
            
            avg_val_loss = val_loss / val_total
            val_accuracy = val_correct / val_total
            val_losses.append(avg_val_loss)
            val_accuracies.append(val_accuracy)
            logging.info(f"Epoch [{epoch + 1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}")

            end_time = time.time()
            epoch_duration = end_time - start_time
            logging.info(f"Epoch [{epoch + 1}/{num_epochs}] completed in {epoch_duration:.2f} seconds.")
            
            # Plot training and validation loss
            plt.figure(figsize=(12, 5))
            plt.subplot(1, 2, 1)
            plt.plot(train_losses, label='Train Loss')
            plt.plot(val_losses, label='Validation Loss')
            plt.xlabel('Epoch')
            plt.ylabel('Loss')
            plt.legend()
            plt.title('Training and Validation Loss')

            # Plot training and validation accuracy
            plt.subplot(1, 2, 2)
            plt.plot(train_accuracies, label='Train Accuracy')
            plt.plot(val_accuracies, label='Validation Accuracy')
            plt.xlabel('Epoch')
            plt.ylabel('Accuracy')
            plt.legend()
            plt.title('Training and Validation Accuracy')

            plt.tight_layout()
            plt.savefig(f'training_epoch_{epoch + 1}.png')
            plt.close()
            logging.info(f"Training and validation metrics for epoch {epoch + 1} saved as training_epoch_{epoch + 1}.png")
    
    # Training the model
    print("\nStarting model training...")
    logging.info("Starting model training...")
    val_loader = test_loader  # In practice, you'd use a separate validation set
    train_model(model, train_loader, val_loader)

    print("Training completed.")
    logging.info("Training completed.")
