import numpy as np
import pandas as pd
import ast
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaModel
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings('ignore')


# Path to MELD dataset
MELD_PATH = r"C:\Users\armaa\Downloads\dataset\MELD\MELD.xlsx"

# Load the data
data = pd.read_excel(MELD_PATH)

# Function to parse string representations of lists
def parse_feature(feature_str):
    return np.array(ast.literal_eval(feature_str))

# Apply parsing to feature columns
for feature in ['V1', 'V2', 'V3', 'V4', 'A2']:
    data[feature] = data[feature].apply(parse_feature)

# Handle class imbalance
classes = data['Label'].unique()
class_weights = compute_class_weight('balanced', classes=classes, y=data['Label'])
class_weights = torch.tensor(class_weights, dtype=torch.float)

# Map labels to indices
label2idx = {label: idx for idx, label in enumerate(classes)}
idx2label = {idx: label for label, idx in label2idx.items()}
data['Label_idx'] = data['Label'].map(label2idx)




class MELDDataset(Dataset):
    def __init__(self, data):
        self.V1 = list(data['V1'])
        self.V2 = list(data['V2'])
        self.V3 = list(data['V3'])
        self.V4 = list(data['V4'])
        self.A2 = list(data['A2'])
        self.labels = data['Label_idx'].values

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        V1 = self.V1[idx]
        V2 = self.V2[idx]
        V3 = self.V3[idx]
        V4 = self.V4[idx]
        A2 = self.A2[idx]
        label = self.labels[idx]
        sample = {'V1': V1, 'V2': V2, 'V3': V3, 'V4': V4, 'A2': A2, 'label': label}
        return sample

# Split data into training and validation sets
train_data, val_data = train_test_split(
    data, test_size=0.2, stratify=data['Label_idx'], random_state=42
)

# Create datasets and dataloaders
train_dataset = MELDDataset(train_data)
val_dataset = MELDDataset(val_data)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)



# Swish Activation Function
class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)

# Mish Activation Function
class Mish(nn.Module):
    def forward(self, x):
        return x * torch.tanh(F.softplus(x))

# Squeeze-and-Excitation Block
class SEBlock(nn.Module):
    def __init__(self, channels, reduction=16):
        super(SEBlock, self).__init__()
        self.fc1 = nn.Linear(channels, channels // reduction)
        self.fc2 = nn.Linear(channels // reduction, channels)

    def forward(self, x):
        y = x.mean(dim=2)
        y = F.relu(self.fc1(y))
        y = torch.sigmoid(self.fc2(y)).unsqueeze(2)
        return x * y


class ResidualTCN(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, dilation):
        super(ResidualTCN, self).__init__()
        self.conv = nn.Conv1d(
            in_channels,
            out_channels,
            kernel_size,
            padding=(kernel_size - 1) * dilation // 2,
            dilation=dilation,
        )
        self.bn = nn.BatchNorm1d(out_channels)
        self.prelu = nn.PReLU()
        self.downsample = (
            nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None
        )

    def forward(self, x):
        y = self.conv(x)
        y = self.bn(y)
        y = self.prelu(y)
        if self.downsample:
            x = self.downsample(x)
        return y + x



class AudioSubNet(nn.Module):
    def __init__(self):
        super(AudioSubNet, self).__init__()
        self.conv1 = nn.Conv1d(1, 256, kernel_size=3, padding=1)
        self.swish = Swish()
        self.bn1 = nn.BatchNorm1d(256)
        self.dropout1 = nn.Dropout(0.2)
        self.se_block = SEBlock(256)
        self.tcn_layers = nn.ModuleList(
            [
                ResidualTCN(256, 256, kernel_size=3, dilation=1),
                ResidualTCN(256, 256, kernel_size=3, dilation=2),
                ResidualTCN(256, 256, kernel_size=3, dilation=4),
            ]
        )
        self.bi_gru = nn.GRU(
            input_size=256, hidden_size=256, num_layers=1, bidirectional=True, batch_first=True, dropout=0.3
        )
        self.attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)
        self.output_layer = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256))

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.conv1(x)
        x = self.swish(x)
        x = self.bn1(x)
        x = self.dropout1(x)
        x = self.se_block(x)
        for layer in self.tcn_layers:
            x = layer(x)
        x = x.permute(0, 2, 1)
        x, _ = self.bi_gru(x)
        x = x.permute(1, 0, 2)
        x, _ = self.attention(x, x, x)
        x = x.permute(1, 0, 2).mean(dim=1)
        x = self.output_layer(x)
        return x


class TextSubNet(nn.Module):
    def __init__(self):
        super(TextSubNet, self).__init__()
        self.conv1 = nn.Conv1d(1, 256, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.bi_lstm = nn.LSTM(
            input_size=256, hidden_size=256, num_layers=1, bidirectional=True, batch_first=True, dropout=0.3
        )
        self.attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)
        self.output_layer = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256))

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.conv1(x)
        x = self.relu(x)
        x = x.permute(0, 2, 1)
        x, _ = self.bi_lstm(x)
        x = x.permute(1, 0, 2)
        x, _ = self.attention(x, x, x)
        x = x.permute(1, 0, 2).mean(dim=1)
        x = self.output_layer(x)
        return x




class VisualSubNet(nn.Module):
    def __init__(self):
        super(VisualSubNet, self).__init__()
        self.conv1 = nn.Conv1d(1, 256, kernel_size=3, padding=1)
        self.mish = Mish()
        self.tcn_layers = nn.ModuleList(
            [
                ResidualTCN(256, 256, kernel_size=3, dilation=1),
                ResidualTCN(256, 256, kernel_size=3, dilation=2),
                ResidualTCN(256, 256, kernel_size=3, dilation=4),
            ]
        )
        self.bi_gru = nn.GRU(
            input_size=256, hidden_size=256, num_layers=1, bidirectional=True, batch_first=True, dropout=0.3
        )
        self.attention = nn.MultiheadAttention(embed_dim=512, num_heads=8)
        self.output_layer = nn.Sequential(nn.Linear(512, 256), nn.LayerNorm(256))

    def forward(self, x):
        x = x.unsqueeze(1)
        x = self.conv1(x)
        x = self.mish(x)
        for layer in self.tcn_layers:
            x = layer(x)
        x = x.permute(0, 2, 1)
        x, _ = self.bi_gru(x)
        x = x.permute(1, 0, 2)
        x, _ = self.attention(x, x, x)
        x = x.permute(1, 0, 2).mean(dim=1)
        x = self.output_layer(x)
        return x


class FusionNetwork(nn.Module):
    def __init__(self, num_classes):
        super(FusionNetwork, self).__init__()
        self.cross_attention = nn.MultiheadAttention(embed_dim=256, num_heads=8)
        self.gating_network = nn.Sequential(
            nn.Linear(256 * 3, 256),
            nn.ReLU(),
            nn.Linear(256, 3),
            nn.Softmax(dim=1),
        )
        self.transformer_decoder_layer = nn.TransformerDecoderLayer(
            d_model=256, nhead=8, dim_feedforward=1024, activation='relu', dropout=0.2
        )
        self.transformer_decoder = nn.TransformerDecoder(self.transformer_decoder_layer, num_layers=4)
        self.classifier = nn.Sequential(
            nn.Linear(256, 512),
            Swish(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes),
        )

    def forward(self, audio_feat, text_feat, visual_feat):
        audio_feat = audio_feat.unsqueeze(0)
        text_feat = text_feat.unsqueeze(0)
        visual_feat = visual_feat.unsqueeze(0)
        # Cross-modal attention
        attn_audio, _ = self.cross_attention(audio_feat, text_feat, text_feat)
        attn_text, _ = self.cross_attention(text_feat, visual_feat, visual_feat)
        attn_visual, _ = self.cross_attention(visual_feat, audio_feat, audio_feat)
        # Concatenate and gate
        concat_features = torch.cat(
            [attn_audio.squeeze(0), attn_text.squeeze(0), attn_visual.squeeze(0)], dim=1
        )
        modality_weights = self.gating_network(concat_features)
        modality_features = torch.stack(
            [attn_audio.squeeze(0), attn_text.squeeze(0), attn_visual.squeeze(0)], dim=1
        )
        fused_features = (modality_weights.unsqueeze(2) * modality_features).sum(dim=1)
        # Transformer decoder
        fused_features = self.transformer_decoder(fused_features.unsqueeze(0), fused_features.unsqueeze(0))
        fused_features = fused_features.squeeze(0)
        # Classification
        output = self.classifier(fused_features)
        return output


class MultimodalEmotionRecognitionModel(nn.Module):
    def __init__(self, num_classes):
        super(MultimodalEmotionRecognitionModel, self).__init__()
        self.audio_subnet = AudioSubNet()
        self.text_subnet = TextSubNet()
        self.visual_subnet = VisualSubNet()
        self.fusion_network = FusionNetwork(num_classes)

    def forward(self, audio_input, text_input, visual_input):
        audio_feat = self.audio_subnet(audio_input)
        text_feat = self.text_subnet(text_input)
        visual_feat = self.visual_subnet(visual_input)
        output = self.fusion_network(audio_feat, text_feat, visual_feat)
        return output



# Instantiate the model
num_classes = len(classes)
model = MultimodalEmotionRecognitionModel(num_classes=num_classes)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Define the loss function with class weights
criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))

# Optimizer and learning rate scheduler
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-5)
scheduler = torch.optim.lr_scheduler.OneCycleLR(
    optimizer, max_lr=1e-3, steps_per_epoch=len(train_loader), epochs=150
)



num_epochs = 150
train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    for batch in train_loader:
        optimizer.zero_grad()
        V1 = torch.tensor(batch['V1'], dtype=torch.float32).to(device)
        V2 = torch.tensor(batch['V2'], dtype=torch.float32).to(device)
        V3 = torch.tensor(batch['V3'], dtype=torch.float32).to(device)
        V4 = torch.tensor(batch['V4'], dtype=torch.float32).to(device)
        A2 = torch.tensor(batch['A2'], dtype=torch.float32).to(device)
        labels = batch['label'].long().to(device)

        # Prepare audio input
        audio_input = torch.cat([V1, V3, V4], dim=1)

        # Forward pass
        outputs = model(audio_input, V2, A2)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()

        running_loss += loss.item() * labels.size(0)

    epoch_loss = running_loss / len(train_dataset)
    train_losses.append(epoch_loss)

    # Validation
    model.eval()
    val_running_loss = 0.0
    with torch.no_grad():
        for batch in val_loader:
            V1 = torch.tensor(batch['V1'], dtype=torch.float32).to(device)
            V2 = torch.tensor(batch['V2'], dtype=torch.float32).to(device)
            V3 = torch.tensor(batch['V3'], dtype=torch.float32).to(device)
            V4 = torch.tensor(batch['V4'], dtype=torch.float32).to(device)
            A2 = torch.tensor(batch['A2'], dtype=torch.float32).to(device)
            labels = batch['label'].long().to(device)

            audio_input = torch.cat([V1, V3, V4], dim=1)
            outputs = model(audio_input, V2, A2)
            loss = criterion(outputs, labels)
            val_running_loss += loss.item() * labels.size(0)

    val_epoch_loss = val_running_loss / len(val_dataset)
    val_losses.append(val_epoch_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}], "
          f"Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}")


plt.figure(figsize=(10, 5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss Curves')
plt.legend()
plt.show()


from sklearn.metrics import confusion_matrix, classification_report

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for batch in val_loader:
        V1 = torch.tensor(batch['V1'], dtype=torch.float32).to(device)
        V2 = torch.tensor(batch['V2'], dtype=torch.float32).to(device)
        V3 = torch.tensor(batch['V3'], dtype=torch.float32).to(device)
        V4 = torch.tensor(batch['V4'], dtype=torch.float32).to(device)
        A2 = torch.tensor(batch['A2'], dtype=torch.float32).to(device)
        labels = batch['label'].long().to(device)

        audio_input = torch.cat([V1, V3, V4], dim=1)
        outputs = model(audio_input, V2, A2)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

cm = confusion_matrix(all_labels, all_preds)
sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

print(classification_report(all_labels, all_preds, target_names=classes))


# Calculate accuracy
train_accuracies = []
val_accuracies = []

# Assuming you have code to compute accuracies per epoch
# Plotting the accuracy curves
plt.figure(figsize=(10, 5))
plt.plot(train_accuracies, label='Training Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Accuracy Curves')
plt.legend()
plt.show()
